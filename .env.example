# Ollama Configuration
# URL for the Ollama API endpoint
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use for text generation
# Available models: mistral, llama2, codellama, etc.
# Run 'ollama list' to see installed models
OLLAMA_MODEL=mistral

# Optional: Override default Ollama configuration
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_TOP_P=0.9
# OLLAMA_NUM_CTX=32000
# OLLAMA_NUM_PREDICT=32000

# Application Configuration
# Port for the FastAPI server (default: 8000)
# PORT=8000

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL=INFO

# Optional: GPU/Device Configuration
# Set to 'cuda' to use GPU, 'cpu' to force CPU usage
# DEVICE=auto

# Optional: Rate Limiting
# Minimum seconds between Ollama requests
# MIN_REQUEST_INTERVAL=60
# MAX_RETRIES=3
# RETRY_DELAY=60

# Storage Paths
# Directory for temporary files
# TEMP_DIR=static/temp
# Directory for generated videos
# VIDEO_DIR=static/videos
